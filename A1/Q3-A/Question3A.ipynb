{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import numpy as np\n",
    "from mosestokenizer import *\n",
    "from tqdm import tqdm\n",
    "from joblib import Parallel, delayed\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AUXILIARY FUNCTIONS\n",
    "\n",
    "def getFilesInFolder(mypath):\n",
    "     return [join(mypath, f) for f in listdir(mypath) if isfile(join(mypath, f))]\n",
    "    \n",
    "\n",
    "def getReview(path):\n",
    "    review = \"\"\n",
    "    with open(path, 'r') as rev:\n",
    "        for line in rev:\n",
    "            review += line[:-1]\n",
    "    with MosesTokenizer('en') as tokenize:\n",
    "        return tokenize(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading necessary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING LEXICON\n",
    "\n",
    "positive_lexicon_path = \"lexicon/positive-words.txt\"\n",
    "negative_lexicon_path = \"lexicon/negative-words.txt\"\n",
    "\n",
    "positive_words = []\n",
    "negative_words = []\n",
    "\n",
    "# saving each word into a list, reading each line, and discarding the\n",
    "# newline \\n\n",
    "\n",
    "with open(positive_lexicon_path, 'r', encoding='ISO-8859-1') as pos:\n",
    "    for line in pos:\n",
    "        positive_words.append(line[:-1])\n",
    "        \n",
    "with open(negative_lexicon_path, 'r', encoding='ISO-8859-1') as neg:\n",
    "    for line in neg:\n",
    "        negative_words.append(line[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READING REVIEWS\n",
    "\n",
    "positive_reviews_path = \"reviews/pos/\"\n",
    "negative_reviews_path = \"reviews/neg/\"\n",
    "\n",
    "positive_reviews = []\n",
    "negative_reviews = []\n",
    "\n",
    "# for each positive file\n",
    "for path in tqdm(getFilesInFolder(positive_reviews_path)):\n",
    "    positive_reviews.append(getReview(path))\n",
    "\n",
    "# for each negative file\n",
    "for path in tqdm(getFilesInFolder(negative_reviews_path)):\n",
    "    negative_reviews.append(getReview(path))\n",
    "    \n",
    "positive_reviews = np.array(positive_reviews)\n",
    "negative_reviews = np.array(negative_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# RANDOMLY SELECT 400 reviews as TEST SET\n",
    "\n",
    "indexes = [True]*600+[False]*400\n",
    "train_indexes = np.random.choice(indexes, 1000, replace=False)\n",
    "test_indexes = np.invert(train_indexes)\n",
    "\n",
    "pos_train = positive_reviews[train_indexes]\n",
    "neg_train = negative_reviews[train_indexes]\n",
    "\n",
    "\n",
    "pos_test = positive_reviews[test_indexes]\n",
    "neg_test = negative_reviews[test_indexes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASSIFER BY WORD COUNTS\n",
    "\n",
    "def classifySentimentByCounting(review):\n",
    "    positives = 0\n",
    "    negatives = 0\n",
    "    \n",
    "    for word in review:\n",
    "        if word in positive_words:\n",
    "            positives += 1\n",
    "        elif word in negative_words:\n",
    "            negatives += 1\n",
    "    return positives >= negatives        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KEEPING TRACK OF TP, FP, TN and FN, ON THE TEST SET, AND USING THEM FOR THE F-SCORE EVALUATION\n",
    "\n",
    "pos_results = Parallel(n_jobs=-1)(delayed(classifySentimentByCounting)(review) for review in tqdm(pos_test))\n",
    "neg_results = Parallel(n_jobs=-1)(delayed(classifySentimentByCounting)(review) for review in tqdm(neg_test))\n",
    "\n",
    "TP = sum(pos_results)\n",
    "FP = len(pos_results) - TP\n",
    "\n",
    "FN = sum(neg_results)\n",
    "TN = len(neg_results) - FN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATING F-SCORE & ACCURACY\n",
    "accuracy = (TP+TN) / (TP+TN+FN+FP)\n",
    "f_score = TP / (TP + 0.5*(FP+FN))\n",
    "print(\"Accuracy on test set: {0:.4f}\".format(accuracy))\n",
    "print(\"F-score on test set: {0:.4f}\".format(f_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running with 10 different splits to obtain a more valid estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RANDOMLY SELECT 400 reviews as TEST SET\n",
    "\n",
    "def runCountingClassification(pos_train, neg_train, pos_test, neg_test):\n",
    "    \n",
    "    # CLASSIFER BY WORD COUNTS\n",
    "\n",
    "    # KEEPING TRACK OF TP, FP, TN and FN, ON THE TEST SET, AND USING THEM FOR THE F-SCORE EVALUATION\n",
    "\n",
    "    pos_results = Parallel(n_jobs=-1)(delayed(classifySentimentByCounting)(review) for review in tqdm(pos_test))\n",
    "    neg_results = Parallel(n_jobs=-1)(delayed(classifySentimentByCounting)(review) for review in tqdm(neg_test))\n",
    "\n",
    "    TP = sum(pos_results)\n",
    "    FP = len(pos_results) - TP\n",
    "\n",
    "    FN = sum(neg_results)\n",
    "    TN = len(neg_results) - FN   \n",
    "\n",
    "    accuracy = (TP+TN) / (TP+TN+FN+FP)\n",
    "    f_score = TP / (TP + 0.5*(FP+FN))\n",
    "    print(\"Accuracy count: \\t{:.4f}\".format(accuracy))\n",
    "    print(\"F1-score: \\t{:.4f}\".format(f_score))\n",
    "    return accuracy, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    print(\"Run {}: \".format(i), end='')\n",
    "    acc, f1 = runCountingClassification()\n",
    "    accuracies.append(acc)\n",
    "    f_scores.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Accuracy on test set: {:.4f} +/- {:.4f}\".format(np.mean(accuracies), np.std(accuracies)))\n",
    "print(\"F-score on test set: {:.4f} +/- {:.4f}\".format(np.mean(f_scores), np.std(f_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task (ii): Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADAPTING the data for the regressor, creating labels\n",
    "\n",
    "def runLogisticRegression(pos_train, neg_train, pos_test, neg_test):\n",
    "    \n",
    "    X_train = np.concatenate([pos_train, neg_train])\n",
    "    y_train = [1]*len(pos_train) + [0]*len(neg_train)\n",
    "\n",
    "    X_test = np.concatenate([pos_test, neg_test])\n",
    "    y_test = [1]*len(pos_test) + [0]*len(neg_test)\n",
    "    # training the vectorizer and model on test data\n",
    "\n",
    "    def dummy(doc):\n",
    "        return doc\n",
    "\n",
    "    vectorizer = CountVectorizer(tokenizer=dummy, preprocessor=dummy)\n",
    "    vectorizer.fit(X_train)\n",
    "\n",
    "    clf = LogisticRegression(random_state=42).fit(vectorizer.transform(X_train), y_train)\n",
    "\n",
    "    y_pred = clf.predict(vectorizer.transform(X_test))\n",
    "    print(\"\\tAccuracy on test: {:.4f}\".format(accuracy_score(y_test, y_pred)))\n",
    "    print(\"\\tF1-score on test: {:.4f}\".format(f1_score(y_test, y_pred)))\n",
    "    \n",
    "    return accuracy_score(y_test, y_pred), f1_score(y_test, y_pred)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "f_scores = []\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    print(\"Run {}: \".format(i))\n",
    "    acc, f1 = runLogisticRegression()\n",
    "    accuracies.append(acc)\n",
    "    f_scores.append(f1)\n",
    "    \n",
    "print(\"Accuracy on test set: {:.4f} +/- {:.4f}\".format(np.mean(accuracies), np.std(accuracies)))\n",
    "print(\"F-score on test set: {:.4f} +/- {:.4f}\".format(np.mean(f_scores), np.std(f_scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paired permutation test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accuracy_pairs = []\n",
    "f1_score_pairs = []\n",
    "\n",
    "for i in range(25):\n",
    "       \n",
    "    indexes = [True]*600+[False]*400\n",
    "    train_indexes = np.random.choice(indexes, 1000, replace=False)\n",
    "    test_indexes = np.invert(train_indexes)\n",
    "\n",
    "    pos_train = positive_reviews[train_indexes]\n",
    "    neg_train = negative_reviews[train_indexes]\n",
    "\n",
    "\n",
    "    pos_test = positive_reviews[test_indexes]\n",
    "    neg_test = negative_reviews[test_indexes]\n",
    "    \n",
    "    print(\"Counting\", i)\n",
    "    acc_count, f1_count = runCountingClassification(pos_train, neg_train, pos_test, neg_test)\n",
    "    acc_log, f1_log = runLogisticRegression(pos_train, neg_train, pos_test, neg_test)\n",
    "    print(\"Logistic\", i)\n",
    "    accuracy_pairs.append([acc_count, acc_log])\n",
    "    f1_score_pairs.append([f1_count, f1_log])   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_pairs = np.array(accuracy_pairs)\n",
    "f1_score_pairs = np.array(f1_score_pairs)\n",
    "\n",
    "print(accuracy_pairs.shape)\n",
    "\n",
    "print(\"F1-score count:\\t {:.4f}\".format(f1_score_pairs[0,0]))\n",
    "print(\"Accuracy count:\\t {:.4f}\".format(accuracy_pairs[0,0]))\n",
    "\n",
    "print(\"F1-score log:\\t {:.4f}\".format(f1_score_pairs[0,1]))\n",
    "print(\"Accuracy log:\\t {:.4f}\".format(accuracy_pairs[0,1]))\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print(\"F1-score count:\\t {:.4f} +/- {:.4f}\".format(np.mean(f1_score_pairs[:,0]), np.std(f1_score_pairs[:,0])))\n",
    "print(\"Accuracy count:\\t {:.4f} +/- {:.4f}\".format(np.mean(accuracy_pairs[:,0]), np.std(accuracy_pairs[:,0])))\n",
    "\n",
    "print(\"F1-score log:\\t {:.4f} +/- {:.4f}\".format(np.mean(f1_score_pairs[:,1]), np.std(f1_score_pairs[:,1])))\n",
    "print(\"Accuracy log:\\t {:.4f} +/- {:.4f}\".format(np.mean(accuracy_pairs[:,1]), np.std(accuracy_pairs[:,1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# average difference in accuracy\n",
    "\n",
    "mean_diff_acc = np.mean(accuracy_pairs[:,1]-accuracy_pairs[:,0])\n",
    "mean_diff_fscore = np.mean(np.array(f1_score_pairs[:,1]-f1_score_pairs[:,0]))\n",
    "\n",
    "n_acc = 0\n",
    "n_f1 = 0\n",
    "\n",
    "n_permutations = 1000000\n",
    "\n",
    "# null hypothesis: they are equal\n",
    "\n",
    "acc_diff = accuracy_pairs[:,1]-accuracy_pairs[:,0]\n",
    "f1_diff = f1_score_pairs[:,1]-f1_score_pairs[:,0]\n",
    "\n",
    "for i in tqdm(range(n_permutations)):\n",
    "    \n",
    "    #shuffle\n",
    "    shuffle_indexes = np.random.choice([1, -1], len(accuracy_pairs), replace=True)\n",
    "    \n",
    "    \n",
    "    if abs(np.mean(acc_diff*shuffle_indexes)) >= abs(mean_diff_acc):\n",
    "        n_acc += 1\n",
    "        \n",
    "    if abs(np.mean(f1_diff*shuffle_indexes)) >= abs(mean_diff_fscore):\n",
    "        n_f1 += 1\n",
    "\n",
    "p_val_acc = (n_acc+mean_diff_acc)/(n_permutations+1)\n",
    "p_val_f1 = (n_f1+mean_diff_fscore)/(n_permutations+1)\n",
    "\n",
    "print(\"Accuracy p-value on {} permutations:\".format(n_permutations), p_val_acc)\n",
    "print(\"F1 score p-value on {} permutations:\".format(n_permutations), p_val_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondab13da5e621634d93b182c53a95930942"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
